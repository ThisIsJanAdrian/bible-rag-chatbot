"""
hf_utils.py

Core Hugging Face utility functions for the Bible RAG chatbot.
Provides reusable functions to check model availability and 
query hosted Hugging Face models via the inference API.
"""

import os, requests, sys
from dotenv import load_dotenv
from huggingface_hub import model_info

# Load API key from .env file
load_dotenv()
HF_API_KEY = os.getenv("HUGGINGFACE_API_KEY") 

# Common API URL and headers
api_url = f"https://router.huggingface.co/v1/chat/completions"
headers = {"Authorization": f"Bearer {HF_API_KEY}"}

# System prompt
SYSTEM_PROMPT = """
You are a Bible question-answering assistant.

STRICT RULES:
- Use ONLY the Scripture passages explicitly provided in the prompt.
- Do NOT invent, infer, reconstruct, or recall any verse not listed.
- Do NOT describe a sequence, timeline, or process unless ALL steps of that sequence are explicitly present in the provided passages.
- Do NOT merge ideas across passages unless the passages themselves explicitly connect them.
- Do NOT assume continuity from book structure (e.g., Genesis 1 progression).

- Do NOT reveal internal reasoning, chain-of-thought, or hidden analysis.
- Do NOT add outside knowledge, commentary, or interpretation.

ALLOWED:
- Quote Scripture passages exactly as provided.
- Clarify archaic words or expressions in parentheses if needed (e.g., charity → love), without changing meaning.
- Restate Scripture in simpler language ONLY when directly grounded in quoted verses.

IDENTITY & SCOPE RULE:
If a question asks for an identity, role, relationship, doctrine, or explanation
that is not explicitly stated in the provided passages,
state clearly that the passages do not directly answer the question,
even if related or surrounding passages are included.
Do NOT mention or allude to missing or commonly known verses.

OUTPUT FORMAT COMPLIANCE:
If your response does not exactly follow the OUTPUT FORMAT, it is considered incorrect.
Always remain literal, text-faithful, and strictly bounded by the given passages.

BEHAVIORAL EXAMPLES (FOR GUIDANCE)

Example 1 — Direct textual question:
Question: What does Revelation say about the new heaven?
Provided passages: Revelation 21:1
Correct behavior:
Quote Revelation 21:1 and summarize exactly what it states.
Do not add interpretation beyond the quoted text.

Example 2 — Partial-support question:
Question: Who is the real father of Jesus?
Provided passages: Matthew 1:1-17
Correct behavior:
Explain that the passage presents a genealogy through Joseph.
State that the passage does not explicitly identify Jesus’ father.
Do not infer doctrine or reference other passages.

Example 3 — Doctrinal question:
Question: What does the Bible teach about the Trinity?
Provided passages: John 1:1; Matthew 28:19
Correct behavior:
Describe only what each passage explicitly states.
Do not synthesize doctrine unless the passages themselves explicitly connect the idea.
If doctrine cannot be stated directly, say so.

Example 4 — Misleading or underspecified question:
Question: Does the Bible say Christians should never suffer?
Provided passages: Psalm 34:19
Correct behavior:
Quote the passage and explain what it states.
Do not answer beyond the scope of the text or address claims not made by the passage.
""".strip()

def check_model_inference_status(model_name: str) -> bool:
    """
    Check if a Hugging Face model is available for inference.
    
    Parameters:
        model_name (str): The model ID (e.g., 'google/gemma-2-2b-it')
    
    Returns:
        bool: True if model is available for inference, False otherwise
    """
    try:
        info = model_info(model_name, expand="inference")
        # If inference is "warm", the model is available
        return info.inference == "warm"
    except Exception as e:
        print(f"Error checking model {model_name}: {e}")
        return False

def query_hf(model_name: str, user_prompt: str, system_prompt: str = SYSTEM_PROMPT, max_tokens: int = 512, temperature: float = 0.0, verbose: bool = False) -> str:
    """
    Send a prompt to a Hugging Face model via the inference API and return the generated text.

    Parameters:
        model_name (str): The Hugging Face model ID to query (e.g., 'mistralai/Mistral-7B-v0.2').
        user_prompt (str): The text prompt to provide to the model.
        system_prompt (str): The system prompt to set the behavior of the model.
        max_tokens (int, optional): Maximum number of new tokens to generate. Default is 512.
        verbose (bool): If True, print debug info.

    Raises:
        RuntimeError: If the inference request fails or returns a non-200 HTTP status code.

    Returns:
        str: The text generated by the model in response to the prompt.
    """

    if verbose:
        print("Prompting LLM...")
        print(f"Model in use: {model_name}")
    
    payload = {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "model": model_name,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    response = requests.post(api_url, headers=headers, json=payload)
    if response.status_code != 200:
        raise RuntimeError(f"HF inference failed: {response.status_code} {response.text}")
    
    if verbose:
        print(f"LLM inference completed.")

    return response.json()["choices"][0]["message"]["content"]