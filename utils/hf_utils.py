"""
hf_utils.py

Core Hugging Face utility functions for the Bible RAG chatbot.
Provides reusable functions to check model availability and 
query hosted Hugging Face models via the inference API.
"""

import os, requests, sys
from dotenv import load_dotenv
from huggingface_hub import model_info

# Load API key from .env file
load_dotenv()
HF_API_KEY = os.getenv("HUGGINGFACE_API_KEY") 

# Common API URL and headers
api_url = f"https://router.huggingface.co/v1/chat/completions"
headers = {"Authorization": f"Bearer {HF_API_KEY}"}

# System prompt
SYSTEM_PROMPT = """
You are a Bible study assistant.

Your task is to answer questions using ONLY the Scripture passages provided in the context.
Do NOT use any outside knowledge, assumptions, or additional verses.
Do NOT summarize or paraphrase beyond what is in the provided passages.
- Quote Scripture exactly as it appears in the passages.
- Include the exact book, chapter, and verse from the provided context for each quote.
- Do not infer identities or merge characters. If multiple people have the same name, treat them as distinct.
- If the answer cannot be fully determined from the provided passages, respond exactly: "The Bible does not provide an answer in the given passages."
- Your explanation should cite only the provided passages, showing how they support the answer, without adding extra interpretation.

Always format your response exactly as follows:

Answer:
<Direct quotation of Scripture from the provided passages that answers the question>

Verse reference:
<List the book, chapter, and verse ranges exactly as in the provided passages>

Explanation:
<Describe how the cited passages support the answer, using only the text in the passages>
""".strip()

def check_model_inference_status(model_name: str) -> bool:
    """
    Check if a Hugging Face model is available for inference.
    
    Parameters:
        model_name (str): The model ID (e.g., 'google/gemma-2-2b-it')
    
    Returns:
        bool: True if model is available for inference, False otherwise
    """
    try:
        info = model_info(model_name, expand="inference")
        # If inference is "warm", the model is available
        return info.inference == "warm"
    except Exception as e:
        print(f"Error checking model {model_name}: {e}")
        return False

def query_hf(model_name: str, prompt: str, max_tokens: int = 512, temperature: float = 0.0) -> str:
    """
    Send a prompt to a Hugging Face model via the inference API and return the generated text.

    Parameters:
        model_name (str): The Hugging Face model ID to query (e.g., 'mistralai/Mistral-7B-v0.2').
        prompt (str): The text prompt to provide to the model.
        max_tokens (int, optional): Maximum number of new tokens to generate. Default is 512.

    Raises:
        RuntimeError: If the inference request fails or returns a non-200 HTTP status code.

    Returns:
        str: The text generated by the model in response to the prompt.
    """
    
    payload = {
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        "model": model_name,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    response = requests.post(api_url, headers=headers, json=payload)
    if response.status_code != 200:
        raise RuntimeError(f"HF inference failed: {response.status_code} {response.text}")

    return response.json()["choices"][0]["message"]["content"].strip()