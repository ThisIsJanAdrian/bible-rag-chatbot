"""
hf_utils.py

Core Hugging Face utility functions for the Bible RAG chatbot.
Provides reusable functions to check model availability and 
query hosted Hugging Face models via the inference API.
"""

import os, requests, sys
from dotenv import load_dotenv

load_dotenv()
HF_API_KEY = os.getenv("HUGGINGFACE_API_KEY") # Load API key from .env file

def check_hf_model(model_name: str):
    """
    Check if a Hugging Face model is available for inference and validate the API key.

    Parameters:
        model_name (str): The Hugging Face model ID to check (e.g., 'mistralai/Mistral-7B-v0.2').

    Raises:
        RuntimeError: If the model is not reachable, the API key is invalid, or another HTTP error occurs.

    Returns:
        None: Prints a confirmation message if the model is accessible.
    """
    api_url = f"https://router.huggingface.co/models/{model_name}"
    headers = {"Authorization": f"Bearer {HF_API_KEY}"}

    try:
        response = requests.get(api_url, headers=headers)
        if response.status_code == 200:
            print(f"[OK] Model '{model_name}' is ready for inference!")
        elif response.status_code == 401:
            raise RuntimeError("Unauthorized: Check your Hugging Face API key.")
        elif response.status_code == 404:
            raise RuntimeError(f"Model '{model_name}' not found. Check the model ID.")
        else:
            raise RuntimeError(f"Unexpected response {response.status_code}: {response.text}")
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"Error connecting to Hugging Face: {e}")

def query_hf(model_name: str, prompt: str, max_tokens: int = 512) -> str:
    """
    Send a prompt to a Hugging Face model via the inference API and return the generated text.

    Parameters:
        model_name (str): The Hugging Face model ID to query (e.g., 'mistralai/Mistral-7B-v0.2').
        prompt (str): The text prompt to provide to the model.
        max_tokens (int, optional): Maximum number of new tokens to generate. Default is 512.

    Raises:
        RuntimeError: If the inference request fails or returns a non-200 HTTP status code.

    Returns:
        str: The text generated by the model in response to the prompt.
    """
    api_url = f"https://router.huggingface.co/models/{model_name}"
    headers = {"Authorization": f"Bearer {HF_API_KEY}"}
    payload = {"inputs": prompt, "parameters": {"max_new_tokens": max_tokens}}

    response = requests.post(api_url, headers=headers, json=payload)
    if response.status_code != 200:
        raise RuntimeError(f"HF inference failed: {response.status_code} {response.text}")

    return response.json()[0]["generated_text"].strip()