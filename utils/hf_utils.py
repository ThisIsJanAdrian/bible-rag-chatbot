"""
hf_utils.py

Core Hugging Face utility functions for the Bible RAG chatbot.
Provides reusable functions to check model availability and 
query hosted Hugging Face models via the inference API.
"""

import os, requests, sys
from dotenv import load_dotenv
from huggingface_hub import model_info

# Load API key from .env file
load_dotenv()
HF_API_KEY = os.getenv("HUGGINGFACE_API_KEY") 

# Common API URL and headers
api_url = f"https://router.huggingface.co/v1/chat/completions"
headers = {"Authorization": f"Bearer {HF_API_KEY}"}

# System prompt
SYSTEM_PROMPT = """
You are a Bible question-answering assistant.

Your task is to answer the user's question using ONLY the Scripture passages explicitly provided in the prompt.

STRICT RULES:
- Use ONLY the Scripture passages provided.
- Do NOT invent, infer, recall, or reference any verse not listed.
- Do NOT add theology, commentary, or outside knowledge.
- Do NOT merge ideas across passages unless the passages themselves explicitly describe the same subject or event.
- Do NOT assume continuity from surrounding chapters or book structure.

TEXTUAL FIDELITY:
- Quote Scripture exactly as provided, without paraphrasing.
- You may clarify archaic terms in parentheses (e.g., charity â†’ love) ONLY if the meaning is directly evident from the text.
- You may restate Scripture in simpler language ONLY when directly grounded in the quoted verses.

MINIMALITY & COMPLETENESS:
- Quote ONLY passages that directly answer the question.
- Use the FEWEST passages necessary.
- Prefer ONE passage if it fully answers the question.
- Use NO MORE THAN THREE passages total unless a single event or opposition is described across multiple passages.
- Do NOT include background or loosely related verses.

SCOPE & LIMITS:
If the provided passages do not explicitly answer the question,
state clearly that the passages do not directly answer it.
Do NOT speculate or reference commonly known verses that are not provided.

OUTPUT FORMAT COMPLIANCE:
If your response does not exactly follow the required OUTPUT FORMAT, it is considered incorrect.
Do NOT reveal internal reasoning, chain-of-thought, or analysis.
Always remain literal, text-faithful, and strictly bounded by the provided passages.
""".strip()

def check_model_inference_status(model_name: str) -> bool:
    """
    Check if a Hugging Face model is available for inference.
    
    Parameters:
        model_name (str): The model ID (e.g., 'google/gemma-2-2b-it')
    
    Returns:
        bool: True if model is available for inference, False otherwise
    """
    try:
        info = model_info(model_name, expand="inference")
        # If inference is "warm", the model is available
        return info.inference == "warm"
    except Exception as e:
        print(f"Error checking model {model_name}: {e}")
        return False

def query_hf(model_name: str, user_prompt: str, system_prompt: str = SYSTEM_PROMPT, max_tokens: int = 512, temperature: float = 0.0, verbose: bool = False) -> str:
    """
    Send a prompt to a Hugging Face model via the inference API and return the generated text.

    Parameters:
        model_name (str): The Hugging Face model ID to query (e.g., 'mistralai/Mistral-7B-v0.2').
        user_prompt (str): The text prompt to provide to the model.
        system_prompt (str): The system prompt to set the behavior of the model.
        max_tokens (int, optional): Maximum number of new tokens to generate. Default is 512.
        verbose (bool): If True, print debug info.

    Raises:
        RuntimeError: If the inference request fails or returns a non-200 HTTP status code.

    Returns:
        str: The text generated by the model in response to the prompt.
    """

    if verbose:
        print("Prompting LLM...")
        print(f"Model in use: {model_name}")
    
    payload = {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "model": model_name,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    response = requests.post(api_url, headers=headers, json=payload)
    if response.status_code != 200:
        raise RuntimeError(f"HF inference failed: {response.status_code} {response.text}")
    
    if verbose:
        print(f"LLM inference completed.")

    return response.json()["choices"][0]["message"]["content"]